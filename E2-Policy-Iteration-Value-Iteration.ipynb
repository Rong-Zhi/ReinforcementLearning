{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP and Dynammic Programming\n",
    "\n",
    "Markov decision processes formally describe an environment for reinforcement learning, where the environment is fully observable.\n",
    "\n",
    "Almost all RL problems can be formalised as MDPs, e.g.\n",
    "Optimal control primarily deals with continuous MDPs, \n",
    "Partially observable problems can be converted into MDPs, \n",
    "Bandits are MDPs with one state\n",
    "\n",
    "Dynammic programming is one of the ways to solve MDP, which assumes full knowledge of the MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GridWord -- Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import  savefig\n",
    "import warnings\n",
    "# if \"../\" not in sys.path:\n",
    "#   sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "from lib.envs.proGenerator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 0, 0.0, True)],\n",
       " 1: [(1.0, 0, 0.0, True)],\n",
       " 2: [(1.0, 0, 0.0, True)],\n",
       " 3: [(1.0, 0, 0.0, True)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration consists of two parts --- Policy Evaluation and Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Policy evalutation\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # TODO: Implement!\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at all possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # for each action, look at the possible next states:\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # calculate the (expected)value function \n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "# Define Policy improvement\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Implement this!\n",
    "        # evaluate current policy\n",
    "        V = policy_eval_fn(policy,env,discount_factor)\n",
    "        # will be set to false if the policy changes\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            # take the action with under 'current policy'\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # find the best action by one-step look ahead\n",
    "            action_values = np.zeros(env.nA)\n",
    "            \n",
    "            \n",
    "        break\n",
    "    \n",
    "    return policy, np.zeros(env.nS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

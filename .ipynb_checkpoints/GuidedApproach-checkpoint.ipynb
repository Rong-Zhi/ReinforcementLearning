{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided RL Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we want to improve the existed RL algorithems in POMDP environments by proposing a guided learning way. \n",
    "\n",
    "Be specific, we firstly generate sequences in MDP environment: [$O_1, A_1, Rwd_1, Done_1, Vpred_1$] with real states from Guided network, and then generate sequences in POMDP environment:[$O_2, A_2, Rwd_2, Done_2, Vpred_2$] with noisy sensor data from Learning network. \n",
    "\n",
    "Feed $O_2$ into Guided net, we could get $A_2'$, and feed $O_1$ into Learning net, we could get $A_1'$. \n",
    "\n",
    "TRPO and PPO are trying to solve the trust region problem by denoting the surrogain loss function as\n",
    "\n",
    "\n",
    "<center>$L(\\theta)=E[\\dfrac{\\pi(a|s)}{\\pi_{old}(a|s)}A]$<center>\n",
    "\n",
    "Where $A_t = \\delta_t+(\\lambda\\gamma)\\delta_{t+1}+...+(\\lambda\\gamma)^{T-t+1}\\delta_{T-1}$  and $\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$\n",
    "\n",
    "Then we have \n",
    "\\begin{equation*}\n",
    "L(\\theta_g)=E[\\dfrac{\\pi(a_2|s_2)}{\\pi_{oldt}(a_2|s_2)}A_2],  L(\\theta_t)=E[\\dfrac{\\pi(a_1|s_1)}{\\pi_{oldg}(a_1|s_1)}A_1]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate advantage by vpred (V), rwd, it is easily to see that we can use the reward directly form the sequences, but the predicted value could be considered as the following ways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{matrix}\n",
    "O_1 & -> & V_{pred1} \\\\\n",
    "O_2 & -> & V_{pred2}'\n",
    "\\end{matrix}\n",
    "\\begin{matrix}\n",
    "O_1 & -> & V_{pred1}' \\\\\n",
    "O_2 & -> & V_{pred2}\n",
    "\\end{matrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we firstly choose $V_{gnet} = O_2 -> V_{pred2}$, and $V_{tnet} = O1 -> V_{pred1}$ to calculate advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](\"img/picture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

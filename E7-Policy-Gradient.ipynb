{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the previous exercises, we performed Value-based RL, which learns value function but with implicit policy function (just e-greedy ). We can also parameterize the policy, which is Policy based RL if there's not value function, and is Actor-Critic if we also estimate its value function. Then we can implement gradient ascent approach to improve the model with the direction of more reward.\n",
    "\n",
    "\n",
    "Common choices for the policy function: Softmax for discrete actions, Gaussian parameters for continuous actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE -- Monte-Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We substitude a sample of G_t form an episode for Q(s,a), unbiased but with high variance.\n",
    "\n",
    "![](img/REINFORCE.png)\n",
    "\n",
    "To reduce the variance we can subtract a baseline for G. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

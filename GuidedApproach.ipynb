{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided RL Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we want to improve RL algorithms in POMDP environments by proposing a guided learning way. \n",
    "\n",
    "To be specific, we firstly generate sequences in MDP environment: [$(S_1, h_1), A_1, Rwd_1, Done_1, Vpred_1$] with real states from Guided network, and then generate sequences in POMDP environment:[$h_2, A_2, Rwd_2, Done_2, Vpred_2$] with noisy sensor data from Learning network. \n",
    "\n",
    "Feed $h_2$ into Guided net, we could get $A_2'$, and feed $S_1$ into Learning net, we could get $A_1'$. \n",
    "\n",
    "TRPO and PPO are trying to solve the trust region problem by denoting the surrogain loss function as\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta)=E[\\dfrac{\\pi(a|s)}{\\pi_{old}(a|s)}A]\\\\\n",
    "s.t.:  D_{KL}(\\pi_{old}||\\pi)<\\delta\n",
    "\\end{equation*}\n",
    "\n",
    "Where $A_t = \\delta_t+(\\lambda\\gamma)\\delta_{t+1}+...+(\\lambda\\gamma)^{T-t+1}\\delta_{T-1}$  and $\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$, which was introduced by V. Minh et al.[1].\n",
    "\n",
    "Then we have \n",
    "\\begin{equation*}\n",
    "L(\\theta_g)=E[\\dfrac{\\pi_g(a_2|h_2)}{\\pi_{oldt}(a_2|h_2)}A_2],  L(\\theta_t)=E[\\dfrac{\\pi_t(a_1|s_1,h1)}{\\pi_{oldg}(a_1|s_1,h1)}A_1]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "D_{KLg}(\\pi_{gold}(.|h_2)||\\pi_g(.|h_2))<\\delta,  \n",
    "D_{KLt}(\\pi_{told}(.|s_1,h1)||\\pi_t(.|s_1,h1))<\\delta\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate advantage by vpred (V), rwd, it is easily to see that we can use the reward directly form the sequences, but the predicted value could be considered as the following ways "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{matrix}\n",
    "S_1, h1 & -> & V_{pred1} \\\\\n",
    "h_2 & -> & V_{pred2}'\n",
    "\\end{matrix}\n",
    "\\begin{matrix}\n",
    "S_1, h1 & -> & V_{pred1}' \\\\\n",
    "h_2 & -> & V_{pred2}\n",
    "\\end{matrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO and TRPO in FVRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Firstly try PPO and TRPO seperately in FieldVisionRockSample environment with mapsize: 5x7, observation type: field_vision_full_pos, observation noise: True, history steps:15. We ran each algorithm for 600 iterations and averaged the results over 5 runs.\n",
    "\n",
    "Then ran PPO and TRPO with an additional augmented entropy term, notice that we use variable entropy coefficient instead of a constant value. Specifically, \n",
    "\\begin{equation*}\n",
    "L(\\theta)=E[\\dfrac{\\pi(a|s)}{\\pi_{old}(a|s)}A + c_{ent}H(\\pi(a|s))]\\\\\n",
    "c_{ent}=max(0.5 - t / T, 0.01)\\\\\n",
    "\\end{equation*}\n",
    "Where t=iteration times so far,  T=maximum iteration times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results of showed below, PPO could reach higher reward result compared with TRPO, but both of them converge prematurely in POMDP envrionment, however, we could achieve much better resutls when we implement PPO and TRPO with additional entropy term and time-variant entropy coefficient, of which the performance is appreciable in FVRS (compared to COPOS). We calculate Mean episode reward here and denoted as 'EpRewMean' in the following graph, and is denoted as 'AverageReturn' in COPOS result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TRPO and PPO in FieldVisionRockSample](img/comparision-5runs.png)\n",
    "<center>TRPO and PPO in FieldVisionRockSample<center>\n",
    "    \n",
    "![COPOS in FieldVisionRockSample](img/copos-fvrs.png)\n",
    "<center>COPOS in FieldVisionRockSample<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided TRPO and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple choices for value networks, we firstly choose $V_{gnet} = h_2 -> V_{pred2}$, and $V_{tnet} = s_1,h_1 -> V_{pred1}$ to calculate advantage.\n",
    "\n",
    "We then test with the same settings as before, first is TRPO, we observed that it couldn't compute a good step for KL constraint, the reason coulde be that the  KL divergence is calculated by different policy distribution, and thus, is possibly too large sometimes. Then, we try the same settings for PPO and get the following results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Guided TRPO in FieldVisionRockSample](img/trpo-guided1.png)\n",
    "<center>Guided TRPO in FieldVisionRockSample<center>\n",
    "    \n",
    "![Guided PPO in FVRS](img/ppo-guided1.png)\n",
    "<center>Guided PPO in FieldVisionRockSample<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above image that this format of guided learning performs worse than PPO itself. The reason could be PPO clipped the surrogate loss into:\n",
    "\n",
    "\\begin{equation*}\n",
    "L^{CLIP}(\\theta)=E[min(r(\\theta)A, clip(r(\\theta), 1-\\epsilon, 1+\\epsilon)A)]\\\\\n",
    "r(\\theta) = \\dfrac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta old}(a|s)}\n",
    "\\end{equation*}\n",
    "where epsilon is a hyperparameter, say, $\\epsilon=0.2$\n",
    "\n",
    "When we calculate surrogate loss with different policy distribution, the results could be far more than 1, and will be eventually clipped into $(1-\\epsilon,1+\\epsilon)$ \n",
    "\n",
    "Therefore, we have to try other combinations, $V_{gnet} = h_2 -> V_{pred2}'$, and $V_{tnet} = s_1, h_1 -> V_{pred1}'$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Guided PPO v2 in FVRS](img/ppo-guided2.png)\n",
    "<center>Guided PPO (v2) in FieldVisionRockSample<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we got extremely small values sometimes when we update guided policy, and the general performance is worse than PPO in FVRS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Guided TRPO and PPO with mixed training samples(not finished yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Asynchronous Methods for Deep Reinforcement Learning from V.Mnih et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
